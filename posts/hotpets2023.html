<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-08-01 Thu 15:13 -->
<meta charset="utf-8">
<title>Are PETs and Algorithmetic Accountability at loggerheads?</title>
<meta name="generator" content="Org mode">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="/res/style.css" type="text/css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/javascript" src="/res/MathJax/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<ul><a href="/">Front</a><a href="/posts">Posts</a><a href="/publications.html">Publications</a></ul><hr>
</div>
<div id="content">
<header>
<h1 class="title">Are PETs and Algorithmetic Accountability at loggerheads?</h1>
</header><div class="PREVIEW" id="orge1a7a45">
<p>
Combining MPC and ML with the goal of obtaining something "practical" may not be
as straightforward as it seems. In fact, MPC might exacerbate certain issues
that arise in deployments of ML. As security practitioners, we must be aware of
these potential pitfalls and guide our efforts accordingly.
</p>

<p>
This post is an extension/adaption of a presentation I gave at HotPETs 2023 in
Lausanne.
</p>

</div>

<figure id="Loggerhead">
<img src="/res/loggerhead.jpg" alt="loggerhead"></img>
<figcaption>
<span class="figure-caption">A loggerhead turtle. Unrelated to this post</span>
</figcaption>
</figure>

<div id="outline-container-orgdc07e2d" class="outline-2">
<h2 id="orgdc07e2d">Background</h2>
<div class="outline-text-2" id="text-orgdc07e2d">
<p>
Machine Learning (ML) is more popular than ever for consumers and businesses
alike. ChatGPT have managed to acquire 100 million users in just a few months,
and a recent(-ish) survey by McKinsey states that &gt;50% of companies employ ML in
some form or another.<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup><sup>, </sup><sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>
</p>

<p>
Many of the practical applications of ML involve some kind of sensitive
information. A system used to generate text, for example, will undoubtedly
receive sensitive prompts<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>. Another system may attempt to predict whether a
patient is at risk of disease, and as such will probably involve that patients
medical records.<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>
</p>

<p>
These sensitive applications of ML have fuelled an interest in so-called
"Privacy-Preserving Machine Learning", or PPML, that in a nutshell, are a class
of techniques for securely and privately evaluating ML models.
</p>

<p>
Privacy, however, is not the only issue in applications of ML. And in fact, it
is but a small one. An arguably larger issue is the that of <i>accountability</i> of
the system. Indeed, the main appeal of ML is that it automates a human process
with greater efficiency, and, hopefully, accuracy as well. But when we replace a
human process with that of a machine, then accountability of bad actions becomes
more opaque.
</p>

<figure id="Bias">
<img src="/res/bias.png" alt="bias"></img>
<figcaption>
<span class="figure-caption">News headlines demonstrating a less-than-stellar side of ML</span>
</figcaption>
</figure>

<p>
ML has been demonstrated to be biased in many cases, in particular when it comes
to minority groups. In light of this fact, accountability is even more
important. User's of an ML system are entitled to know if the output they get
from an ML system is actually good, or if they are being unfairly treated.
</p>

<p>
In the EU, The need for accountability is currently being written into the "AI
act"; China is working on a similar law, and maybe the US will someday do as
well. Who knows. In any event, accountability appears to be just as important as
privacy when it comes to ML systems.
</p>
</div>

<div id="outline-container-org76bb4c1" class="outline-3">
<h3 id="org76bb4c1">Title</h3>
<div class="outline-text-3" id="text-org76bb4c1">
<p>
This post is an extension of a talk I gave recently at HotPETs 2023 in
Lausanne. The basic thesis of that talk being that PPML solutions should
consider how to account for accountability as well as privacy. At least from a
glace, achieving accountability seems to imply that agreat deal of insight into
an ML model is required. On the other hand, PPML seemingly removes this
insight. That might cause problems.
</p>

<p>
Slides from the talk can be found <a href="https://anderspkd.github.io/res/files/hotpets2023_slides.pdf">here</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc6fd2ea" class="outline-2">
<h2 id="orgc6fd2ea">Threat modeling for PPML</h2>
<div class="outline-text-2" id="text-orgc6fd2ea">
<p>
At the center of the issue I'm writing about here, is the fact that PPML
operates with <i>two</i> threat models, not one. First, there is whatever threat
model that the privacy tech operates in. And second, there's whatever threat
model the ML model operates in.
</p>

<p>
If we specify each in isolation, then we are effectively talking about two
distinct adversaries that operate independently.
</p>

<p>
This may work. In fact, that's essentially the threat model of a recent cool
<a href="https://files.sri.inf.ethz.ch/website/papers/ccs22-phoenix.pdf">paper</a> by Jovanovic et al.
</p>

<p>
But such a separation is a bit hard to justify in some scenarios. In particular
those where the server is involved in the computation. Let's consider an example
to see what I mean:
</p>

<p>
Suppose we have a single server \(S\) and a client \(C\). The server has a model
\(M\), the client has an input \(x\) and the purpose of the system is to allow the
client to learn \(M(x)\). The client wishes to keep its input private, so it
resorts to using a secure computation protocol with semi-honest security. That
is, it trusts the that the server won't deviate from the protocol.
</p>

<p>
Now, it is important to remember that guarantees about the <i>input</i> is <i>not</i> part
of the protocol. So, <i>technically</i> speaking, the server is allowed to input a
"bad" (e.g., biased) model. And such an action might be perfectly reasonable for
a misbehaving server: Perhaps the server is resigned to not being able to see
the clients inputs&#x2014;by using a biased model, it can still unfairly deny a loan,
insurance or job to targeted clients.
</p>

<p>
If the threat model we are using (for the secure computation) is semi-honest,
then an argument can be made that the server won't use a biased model on
purpose, and so, perhaps we are good.
</p>

<p>
If the threat model on the other hand assumes a malicious adversary, then it
suddenly seems hard to argue that, on the one hand, the server cannot be trusted
to even send the right messages in the protocol, but on the other hand, the
server is trusted to use a non-biased model.
</p>
</div>
</div>

<div id="outline-container-orgfa8ca6a" class="outline-2">
<h2 id="orgfa8ca6a">Solving this conundrum</h2>
<div class="outline-text-2" id="text-orgfa8ca6a">
<p>
Whether or not the above is an issue, it does lend itself to some interesting
research questions: In particular, is it possible to provide <i>both</i> privacy and
some reasonable notion of fairness, robustness, and/or explainability?
</p>

<p>
What makes these questions interesting, is the fact that whatever ML properties
we wish to satisfy, must somehow be satisfiable while working in a secure
computation context. And that seems hard to do, at least intuitively. Indeed,
guaranteeing certain properties about an ML model seemingly implies a great(er)
degree of insight into the model's parameters; but this insight is exactly what
secure computation techniques aim to remove!
</p>

<p>
And while this aparent clash between properties related to accountability and
those related to privacy, have been <a href="https://ieeexplore.ieee.org/document/9933776">noted</a> before, it doesn't mean that the
problem cannot be solved (in some way, shape or form).
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app">https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
<a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/global-survey-the-state-of-ai-in-2021">https://www.mckinsey.com/capabilities/quantumblack/our-insights/global-survey-the-state-of-ai-in-2021</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
For example, multiple example prompts on <a href="https://stackdiary.com/chatgpt/role-based-prompts/">https://stackdiary.com/chatgpt/role-based-prompts/</a> involve potentially private information. Specifically the prompts that attempts to get ChatGPT to act as a health coach or therapist.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
See e.g., <a href="https://www.ibm.com/topics/artificial-intelligence-medicine">https://www.ibm.com/topics/artificial-intelligence-medicine</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<hr><p class="license">CC BY-SA</p><p class="date">Published: 2024-01-01</p><p class="date">Last modified: 2024-08-01 Thu 15:13</p>
</div>
</body>
</html>
